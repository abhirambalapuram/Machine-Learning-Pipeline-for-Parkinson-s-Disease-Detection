# -*- coding: utf-8 -*-
"""ML Pipeline for Parkinson’s Disease Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wGLzydasna9lsvUd9NwWe71OW4q0VdU1
"""

from pathlib import Path
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib

from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score, roc_auc_score, confusion_matrix, classification_report
)
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

csv_name = "/content/parkinsons.csv"
df = pd.read_csv(csv_name)

print("Loaded shape:", df.shape)
df.head()

TARGET = "status"
ID_COLS = ["name"]  # will be dropped if present

# Validating
if TARGET not in df.columns:
    raise ValueError(f"Target column '{TARGET}' not found. Available columns: {list(df.columns)}")

# Auto-dropping non-feature identifiers
df_clean = df.drop(columns=[c for c in ID_COLS if c in df.columns]).copy()

# Dropping rows with any NaNs
df_clean = df_clean.dropna().copy()

print("After cleaning:", df_clean.shape)
print("Columns:", list(df_clean.columns))
df_clean.head()

display(df_clean.describe())

# Missing counts
missing = df_clean.isna().sum().sort_values(ascending=False)
print("Top missing:\n", missing.head(10))

# Correlation heatmap for numeric features
features_only = df_clean.drop(columns=[TARGET]).select_dtypes(include=[np.number])
if features_only.shape[1] > 1:
    corr = features_only.corr(numeric_only=True)

    plt.figure(figsize=(8, 6))
    plt.imshow(corr, interpolation="nearest")
    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)
    plt.yticks(range(len(corr.index)), corr.index)
    plt.title("Feature Correlation Heatmap")
    plt.colorbar()
    plt.tight_layout()
    plt.show()
else:
    print("Not enough numeric features for a correlation heatmap.")

TEST_SIZE = 0.2
SEED = 42

X = df_clean.drop(columns=[TARGET])
y = df_clean[TARGET].astype(int)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, stratify=y, random_state=SEED
)

X_train.shape, X_test.shape, y_train.value_counts(), y_test.value_counts()

def get_models_and_grids(random_state: int = 42):
    models = {
        "logreg": Pipeline([
            ("scaler", StandardScaler()),
            ("clf", LogisticRegression(max_iter=500, solver="liblinear"))
        ]),
        "rf": Pipeline([
            ("clf", RandomForestClassifier(random_state=random_state, n_jobs=-1))
        ]),
        "svc": Pipeline([
            ("scaler", StandardScaler()),
            ("clf", SVC(probability=True, random_state=random_state))
        ]),
    }

    param_grids = {
        "logreg": {
            "clf__C": [0.1, 1.0, 5.0],
            "clf__penalty": ["l1", "l2"]
        },
        "rf": {
            "clf__n_estimators": [200, 400],
            "clf__max_depth": [None, 10, 20]
        },
        "svc": {
            "clf__C": [0.5, 1.0, 2.0],
            "clf__gamma": ["scale", "auto"]
        }
    }
    return models, param_grids

models, grids = get_models_and_grids(SEED)
list(models.keys()), list(grids.keys())

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)

leaderboard = {}
best = {"name": None, "score": -np.inf, "model": None}

for name, pipe in models.items():
    gs = GridSearchCV(
        pipe,
        grids[name],
        cv=cv,
        scoring="roc_auc",
        n_jobs=-1,
        refit=True,
        verbose=0
    )
    gs.fit(X_train, y_train)
    leaderboard[name] = {"best_score_cv": float(gs.best_score_), "params": gs.best_params_}

    if gs.best_score_ > best["score"]:
        best.update({"name": name, "score": float(gs.best_score_), "model": gs.best_estimator_})

print("CV leaderboard:")
for k, v in leaderboard.items():
    print(f"  {k}: AUC={v['best_score_cv']:.4f} | {v['params']}")
print("\nBest model:", best["name"], "CV AUC:", f"{best['score']:.4f}")

def evaluate(model, X_test, y_test, prefix="best"):
    preds = model.predict(X_test)

    # Accuracy, Kappa
    acc = accuracy_score(y_test, preds)
    kappa = metrics.cohen_kappa_score(y_test, preds)

    # AUC
    proba = None
    if hasattr(model, "predict_proba"):
        try:
            proba = model.predict_proba(X_test)[:, 1]
        except Exception:
            pass

    auc = roc_auc_score(y_test, proba) if proba is not None else float("nan")

    # Classification report
    report_text = classification_report(y_test, preds)
    print(f"{prefix} — Accuracy: {acc:.4f}, ROC AUC: {auc:.4f}, Kappa: {kappa:.4f}")
    print("\nClassification report:\n", report_text)

    # Confusion matrix
    cm = confusion_matrix(y_test, preds)
    plt.figure(figsize=(4, 4))
    plt.imshow(cm, interpolation="nearest")
    plt.title(f"Confusion Matrix — {prefix}")
    plt.xticks([0, 1], ["Pred 0", "Pred 1"])
    plt.yticks([0, 1], ["True 0", "True 1"])
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, cm[i, j], ha="center", va="center")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.tight_layout()
    plt.show()

    # JSON serializable metrics
    return {
        "accuracy": float(acc),
        "roc_auc": float(auc),
        "cohen_kappa": float(kappa),
        "confusion_matrix": cm.tolist(),
        "classification_report_text": report_text
    }

test_metrics = evaluate(best["model"], X_test, y_test, prefix=f"best_{best['name']}")
test_metrics

OUTPUT_DIR = Path("./artifacts")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Save model
model_path = OUTPUT_DIR / f"best_model_{best['name']}.joblib"
joblib.dump(best["model"], model_path)

# Save leaderboard + model card
with open(OUTPUT_DIR / "cv_leaderboard.json", "w", encoding="utf-8") as f:
    json.dump(leaderboard, f, indent=2)

model_card = {
    "best_model": best["name"],
    "cv_best_score_auc": best["score"],
    "test_metrics": test_metrics,
    "feature_columns": list(X.columns),
    "target_column": TARGET
}
with open(OUTPUT_DIR / "model_card.json", "w", encoding="utf-8") as f:
    json.dump(model_card, f, indent=2)

print("Saved to:", OUTPUT_DIR.resolve())
print("Model file:", model_path)

# demo
loaded = joblib.load(model_path)
demo_preds = loaded.predict(X_test)
print("Demo predictions (first 10):", demo_preds[:10])